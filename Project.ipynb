{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # for generating file paths\n",
    "import os # for creating folders\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from keplergl import KeplerGl\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "\n",
    "import warnings\n",
    "\n",
    "from typing import List, Tuple, Union, Any, Dict\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_DIRECTORY = 'taxi_data'  # directory to save taxi data\n",
    "TAXI_PARQUET = 'yellow_tripdata_{:04d}-{:02d}.parquet'  # taxi data filename\n",
    "\n",
    "SHAPE_DIRECTORY = 'taxi_zones'  # directory to save shape file\n",
    "SHAPE_FILE = Path(SHAPE_DIRECTORY, 'taxi_zones.shp')  # shape file path\n",
    "UBER_DIRECTORY = 'uber_data'  # directory to save uber data\n",
    "UBER_CSV = Path(UBER_DIRECTORY, 'uber_rides_sample.csv')  # uber file path\n",
    "\n",
    "WEATHER_DIRECTORY = 'weather_data'  # directory to save weather data\n",
    "WEATHER_URL = 'https://www.ncei.noaa.gov/data/local-climatological-data/access/{}/72505394728.csv'  # url to download weather data\n",
    "CLEAN_DIRECTORY = 'cleaned_data'\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "AVG_EARTH_RADIUS_KM = 6371.0088  # average earth radius (km), used in calculating distance acorrding to latitude and longitude\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd56b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders\n",
    "os.makedirs(TAXI_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(SHAPE_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(UBER_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(WEATHER_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(CLEAN_DIRECTORY, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [x] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord: Tuple[float], to_coord: Tuple[float]) -> float:\n",
    "    '''calculate and return the distance\n",
    "\n",
    "    Keyword arguments:\n",
    "    from_coord -- the coordinates we start with\n",
    "    to_coord -- the coordinates we end with\n",
    "    \n",
    "    '''\n",
    "    # starting latitude\n",
    "    lat1, lon1 = from_coord\n",
    "    # ending latitude\n",
    "    lat2, lon2 = to_coord\n",
    "    \n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "    \n",
    "    # calculate the distance of two latitudes and longitudes\n",
    "    lat = lat2 - lat1\n",
    "    lon = lon2 - lon1\n",
    "    \n",
    "    # calculate the distance\n",
    "    d = math.sin(lat * 0.5) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(lon * 0.5) ** 2\n",
    "    \n",
    "    return 2 * AVG_EARTH_RADIUS_KM * math.asin(math.sqrt(d))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(data: pd.DataFrame, lat1_name: str, lat2_name: str, lon1_name: str, lon2_name: str) -> pd.DataFrame:\n",
    "    '''add a distance column to the data and return the data\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- the dataframe we are targeted \n",
    "    lat1_name, lat2_name, lon1_name, lon2_name -- the columns we want to add\n",
    "    \n",
    "    '''\n",
    "    data['distance'] = data[[lat1_name, lat2_name, lon1_name, lon2_name]].apply(\n",
    "        lambda x: calculate_distance((x[lat1_name], x[lon1_name]), (x[lat2_name], x[lon2_name])), axis=1\n",
    "    )\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69327b92",
   "metadata": {},
   "source": [
    "### Processing Shp Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488b4f7",
   "metadata": {},
   "source": [
    "shp file is downloaded via https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page, unzipped mannually, and saved into ./taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111a060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_shp_data() -> pd.DataFrame:\n",
    "    '''load, clean, and return cleaned shp file data'''\n",
    "    shp = gpd.read_file(SHAPE_FILE)\n",
    "    \n",
    "    # convert coordinates of geometric zones to real latitude/longitude coordinates\n",
    "    shp['lat'] = shp.to_crs(4326).centroid.y\n",
    "    shp['lon'] = shp.to_crs(4326).centroid.x\n",
    "    \n",
    "    # select and rename columns\n",
    "    shp = shp[['LocationID', 'lat', 'lon']]\n",
    "    shp.columns = ['location', 'lat', 'lon']\n",
    "    \n",
    "    return shp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836d792",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b82169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file: Union[str, Path]) -> pd.DataFrame:\n",
    "    '''load and clean the uber data\n",
    "\n",
    "    Keyword argument:\n",
    "    csv_file -- file we want to load and clean\n",
    "    \n",
    "    '''\n",
    "    # load uber data\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # select columns\n",
    "    data = data[['pickup_datetime', 'fare_amount', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']]\n",
    "    \n",
    "    # removing invalid data points\n",
    "    data = data[(data['fare_amount'] >= 1) & (data['passenger_count'] >= 1)]\n",
    "    \n",
    "    # normalize the dtype for time column\n",
    "    utc = data['pickup_datetime'].iloc[0][-3:]\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'].str.slice(0, -4), utc=utc)\n",
    "    \n",
    "    # select rides whose pickup location and dropoff location lie in the specified box\n",
    "    data = data[(data['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \\\n",
    "                (data['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \\\n",
    "                (data['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \\\n",
    "                (data['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1]) & \\\n",
    "                (data['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \\\n",
    "                (data['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \\\n",
    "                (data['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \\\n",
    "                (data['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    \n",
    "    # drop duplicates\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    data = data[['pickup_datetime', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'fare_amount', 'passenger_count']]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83938e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    '''return the full uber data with distance columns added'''\n",
    "    \n",
    "    data = load_and_clean_uber_data(UBER_CSV)\n",
    "    data = add_distance_column(data, lat1_name='pickup_latitude', lon1_name='pickup_longitude', lat2_name='dropoff_latitude', lon2_name='dropoff_longitude')\n",
    "    \n",
    "    # regroup to monthly frequency\n",
    "    month_freq = data.resample('M', on='pickup_datetime')['pickup_datetime'].count().to_frame('frequency')\n",
    "    \n",
    "    return data, month_freq\n",
    "\n",
    "# load and clean uber data and count number of rides in monthly uber data\n",
    "uber_data, uber_month_freq = get_uber_data()\n",
    "uber_data.to_parquet(Path(CLEAN_DIRECTORY, 'uber.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57470ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.683325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.457593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>12.9</td>\n",
       "      <td>1</td>\n",
       "      <td>5.036384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.661686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.475456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickup_datetime  pickup_latitude  pickup_longitude  \\\n",
       "0 2015-05-07 19:52:06+00:00        40.738354        -73.999817   \n",
       "1 2009-07-17 20:04:56+00:00        40.728225        -73.994355   \n",
       "2 2009-08-24 21:45:00+00:00        40.740770        -74.005043   \n",
       "3 2009-06-26 08:22:21+00:00        40.790844        -73.976124   \n",
       "4 2014-08-28 17:47:00+00:00        40.744085        -73.925023   \n",
       "\n",
       "   dropoff_latitude  dropoff_longitude  fare_amount  passenger_count  distance  \n",
       "0         40.723217         -73.999512          7.5                1  1.683325  \n",
       "1         40.750325         -73.994710          7.7                1  2.457593  \n",
       "2         40.772647         -73.962565         12.9                1  5.036384  \n",
       "3         40.803349         -73.965316          5.3                3  1.661686  \n",
       "4         40.761247         -73.973082         16.0                5  4.475456  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75cfe834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-31 00:00:00+00:00</th>\n",
       "      <td>2503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-28 00:00:00+00:00</th>\n",
       "      <td>2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03-31 00:00:00+00:00</th>\n",
       "      <td>2665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-30 00:00:00+00:00</th>\n",
       "      <td>2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-31 00:00:00+00:00</th>\n",
       "      <td>2619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           frequency\n",
       "pickup_datetime                     \n",
       "2009-01-31 00:00:00+00:00       2503\n",
       "2009-02-28 00:00:00+00:00       2261\n",
       "2009-03-31 00:00:00+00:00       2665\n",
       "2009-04-30 00:00:00+00:00       2531\n",
       "2009-05-31 00:00:00+00:00       2619"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_month_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Downloading Taxi Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls():\n",
    "    '''return all links about yellow taxi trip records for all years available'''\n",
    "    # fetch the url contents\n",
    "    # the main page\n",
    "    page = requests.get(TAXI_URL)  \n",
    "    page = BeautifulSoup(page.content)\n",
    "    \n",
    "    # find all years\n",
    "    years = page.find_all(class_='faq-answers')  \n",
    "    # find all links about yellow taxi in each year\n",
    "    links = [year.find_all(title='Yellow Taxi Trip Records') for year in years]\n",
    "    # with format List[List[str]], get the links about yellow taxi for all years\n",
    "    links = [[j.get_attribute_list('href')[0] for j in i] for i in links] \n",
    "    \n",
    "    links = reduce(lambda x, y: x + y, links) \n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_taxi_data(url: str) -> None:\n",
    "    '''download the taxi data into the file named taxi_data with url provided'''\n",
    "    # e.g. './taxi_data/yellow_tripdata_2022-01.parquet'\n",
    "    path = Path('taxi_data', url.split('/')[-1])  \n",
    "    \n",
    "    if path.exists():\n",
    "        # already downloaded, skip this file\n",
    "        return\n",
    "    # not downloaded yet, download now\n",
    "    data = requests.get(url)\n",
    "    \n",
    "    # save raw data\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(data.content)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading taxi data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [00:00<00:00, 1492.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_taxi_data() -> None:\n",
    "    '''return raw taxi data'''\n",
    "    # get all taxi data's urls\n",
    "    all_taxi_urls = find_taxi_parquet_urls()\n",
    "    \n",
    "    for parquet_url in tqdm(all_taxi_urls, ncols=120, desc='downloading taxi data'):\n",
    "        data = get_month_taxi_data(parquet_url)\n",
    "        \n",
    "# download taxi data for each month\n",
    "taxi_data = get_taxi_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c1089",
   "metadata": {},
   "source": [
    "### Processing Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd85609",
   "metadata": {},
   "source": [
    "The columns names of taxi data for different year varies:\n",
    "\n",
    "During year 2009, the column names are\n",
    "- ['vendor_name', 'Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime', 'Passenger_Count', 'Trip_Distance', 'Start_Lon', 'Start_Lat', 'Rate_Code', 'store_and_forward', 'End_Lon', 'End_Lat', 'Payment_Type', 'Fare_Amt', 'surcharge', 'mta_tax', 'Tip_Amt', 'Tolls_Amt', 'Total_Amt']\n",
    "\n",
    "During year 2010, the column names are\n",
    "- ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "\n",
    "From year 2011 to now, the column names are\n",
    "- ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
    "\n",
    "And for year 2009 and 2010, the latitidue and longitude of pickup and dropoff location are directly given in the dataset, whereas in year 2011 and after, only location IDs are given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "646ea5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_taxi_data(parquet_file: Union[str, Path], shp: pd.DataFrame, year: int) -> pd.DataFrame:\n",
    "    '''load, clean, and return the cleaned taxi data\n",
    "\n",
    "    Keyword arguments:\n",
    "    parquet_file -- file we want to load and clean\n",
    "    shp -- the file our coordinates based on\n",
    "    year -- data with specific year we wish to clean upon\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # load data\n",
    "    data = pd.read_parquet(parquet_file)\n",
    "    # select and rename columns\n",
    "    if year == 2009:\n",
    "        data = data[['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime', 'Start_Lat', 'Start_Lon', 'End_Lat', 'End_Lon', 'Fare_Amt', 'Tip_Amt', 'Passenger_Count']]\n",
    "        data.columns = ['pickup_datetime', 'dropoff_datetime', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'fare_amount', 'tip_amount', 'passenger_count']\n",
    "    elif year == 2010:\n",
    "        data = data[['pickup_datetime', 'dropoff_datetime', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'fare_amount', 'tip_amount', 'passenger_count']]\n",
    "    else:\n",
    "        data = data[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount', 'passenger_count']]\n",
    "        data.columns = ['pickup_datetime', 'dropoff_datetime', 'pickup_location', 'dropoff_location', 'fare_amount', 'tip_amount', 'passenger_count']\n",
    "    # removing invalid data points\n",
    "    data = data[(data['fare_amount'] >= 1) & (data['tip_amount'] >= 0) & (data['passenger_count'] >= 1)]\n",
    "    # merge with shape file to get specific latitude/longitude coordinates\n",
    "    if year >= 2011:\n",
    "        data = pd.merge(data, shp, left_on='pickup_location', right_on='location')\n",
    "        data = data.rename(columns={'lat': 'pickup_latitude', 'lon': 'pickup_longitude'})\n",
    "        data = pd.merge(data, shp, left_on='dropoff_location', right_on='location')\n",
    "        data = data.rename(columns={'lat': 'dropoff_latitude', 'lon': 'dropoff_longitude'})\n",
    "    # select rides whose pickup location and dropoff location lie in the specified box\n",
    "    data = data[(data['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \\\n",
    "                (data['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \\\n",
    "                (data['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \\\n",
    "                (data['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1]) & \\\n",
    "                (data['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \\\n",
    "                (data['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \\\n",
    "                (data['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \\\n",
    "                (data['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    # select columns\n",
    "    data = data[['pickup_datetime', 'dropoff_datetime', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'fare_amount', 'tip_amount', 'passenger_count']]\n",
    "    # normalize column dtypes\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "    data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])\n",
    "    data['passenger_count'] = data['passenger_count'].astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f5e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_taxi_data(data: pd.DataFrame, n: int, random_state: int = 0) -> pd.DataFrame:\n",
    "    '''randomly sampling taxi data without replacement and return the sampled data\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- file we want to sample from\n",
    "    n -- number of random sampling we wish to perform\n",
    "    random state -- random state our random sampling based on (default = 0)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # the number of samples shall not exceed the total samples in the dataset\n",
    "    n = min(n, data.shape[0])  \n",
    "    \n",
    "    # perform random sampling\n",
    "    data = data.sample(n, replace=False, random_state=random_state)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8375d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_taxi_data(parquet_file: Union[str, Path], shp: pd.DataFrame, n: int, year: int, random_state: int = 0) -> pd.DataFrame:\n",
    "    '''return the monthly taxi data \n",
    "\n",
    "    Keyword arguments:\n",
    "    parquet_file -- file we load in\n",
    "    shp -- the shp file we based on\n",
    "    n -- number of random sampling we wish to perform\n",
    "    year -- specify specific year we want\n",
    "    random state -- random state our random sampling based on (default = 0)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(parquet_file):\n",
    "        print(f'File {parquet_file} not existed, skip this file.')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = load_and_clean_taxi_data(parquet_file=parquet_file, shp=shp, year=year)\n",
    "    data = sample_taxi_data(data=data, n=n, random_state=random_state)\n",
    "    data = add_distance_column(data, lat1_name='pickup_latitude', lon1_name='pickup_longitude', \n",
    "                               lat2_name='dropoff_latitude', lon2_name='dropoff_longitude')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f224ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data(sample_freq, random_state: int = 0) -> pd.DataFrame:\n",
    "    '''return the final cleaned taxi data \n",
    "\n",
    "    Keyword arguments:\n",
    "    sample_freq -- the sampled data with frequency specified\n",
    "    random state -- random state our random sampling based on (default = 0)\n",
    "    \n",
    "    '''    \n",
    "    taxi_data = []\n",
    "    shp = load_and_clean_shp_data()\n",
    "    \n",
    "    for year in range(2009, 2023):\n",
    "        for month in tqdm(range(1, 13), ncols=120, desc=f'getting taxi data of {year:04d}'):\n",
    "            parquet_file = Path(TAXI_DIRECTORY, TAXI_PARQUET.format(year, month))\n",
    "            \n",
    "            try:\n",
    "                n = sample_freq.loc[f'{year:04d}-{month:02d}'].squeeze()\n",
    "           \n",
    "            except KeyError:\n",
    "                # if uber data does not contain this period of time, set n to 2000\n",
    "                n = 2000\n",
    "            \n",
    "            data = get_month_taxi_data(parquet_file=parquet_file, shp=shp, n=n, year=year, random_state=random_state)\n",
    "            taxi_data.append(data)\n",
    "    \n",
    "    taxi_data = pd.concat(taxi_data, axis=0, ignore_index=True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0472736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting taxi data of 2009: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [11:17<00:00, 56.48s/it]\n",
      "getting taxi data of 2010: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [09:43<00:00, 48.65s/it]\n",
      "getting taxi data of 2011: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [07:38<00:00, 38.18s/it]\n",
      "getting taxi data of 2012: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:44<00:00, 43.69s/it]\n",
      "getting taxi data of 2013: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:52<00:00, 44.36s/it]\n",
      "getting taxi data of 2014: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:31<00:00, 42.65s/it]\n",
      "getting taxi data of 2015: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:03<00:00, 30.26s/it]\n",
      "getting taxi data of 2016: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:11<00:00, 25.97s/it]\n",
      "getting taxi data of 2017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:36<00:00, 23.06s/it]\n",
      "getting taxi data of 2018: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [03:12<00:00, 16.04s/it]\n",
      "getting taxi data of 2019: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [02:04<00:00, 10.36s/it]\n",
      "getting taxi data of 2020: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:30<00:00,  2.51s/it]\n",
      "getting taxi data of 2021: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:34<00:00,  2.86s/it]\n",
      "getting taxi data of 2022: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File taxi_data/yellow_tripdata_2022-10.parquet not existed, skip this file.\n",
      "File taxi_data/yellow_tripdata_2022-11.parquet not existed, skip this file.\n",
      "File taxi_data/yellow_tripdata_2022-12.parquet not existed, skip this file.\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_taxi_data(uber_month_freq)\n",
    "\n",
    "taxi_data.to_parquet(Path(CLEAN_DIRECTORY, 'taxi.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a8de5",
   "metadata": {},
   "source": [
    "### Downloading Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1afd3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(start: int = 2005, end: int = 2022) -> None:\n",
    "    '''download weather data to local file\n",
    "\n",
    "    Keyword arguments:\n",
    "    start -- start year (default = 2005)\n",
    "    end -- end year (default = 2022)\n",
    "    \n",
    "    '''\n",
    "    for year in tqdm(range(start, end + 1), ncols=120, desc='downloading weather data'):\n",
    "        path = Path('weather_data', str(year) + '.csv')\n",
    "        \n",
    "        if path.exists():\n",
    "            # already downloaded, skip this file\n",
    "            continue\n",
    "            \n",
    "        # not downloaded yet, download now\n",
    "        data = pd.read_csv(WEATHER_URL.format(year))\n",
    "        \n",
    "        data.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4afdc4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading weather data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 2270.74it/s]\n"
     ]
    }
   ],
   "source": [
    "get_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee532c9c",
   "metadata": {},
   "source": [
    "the weather data contains the following:\n",
    "\n",
    "* HourlyPrecipitation: contains numbers, strings of numbers (such as '0.12s'), NaNs, and 'T'. Need to convert 'T' to 0.0, use regex to extract the numbers, and treat non-number-strings as NaNs\n",
    "\n",
    "* HourlyWindSpeep: contains numbers, NaNs\n",
    "\n",
    "* HourlyDryBuldTemperature: contains numbers, strings of numbers (such as '52.0'), NaNs, and strings of numbers and other chars (such as '81s'). Need to first use regex to extract the numbers, and treat non-number-strings as NaNs\n",
    "\n",
    "* HourlyVisibility: same as HourlyDryBuldTemperature\n",
    "\n",
    "* NaNs policy: sort ascendingly on time, and fill all NaNs with precedent observations (method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc36b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weather_data(x: Union[str, int, float]) -> float:\n",
    "    '''return normalized data; if data does not exist, return NaN\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the value we wish to normalize\n",
    "    \n",
    "    '''\n",
    "    # conver to string\n",
    "    x = str(x)\n",
    "    \n",
    "    # find all the integers and floats\n",
    "    x = re.findall('[\\d|\\.]+', x) \n",
    "    \n",
    "    if len(x) >= 1:\n",
    "        return float(x[0])\n",
    "    else:\n",
    "        # find no numbers\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''return the cleaned hourly weather data\n",
    "\n",
    "    Keyword argument:\n",
    "    data -- the dataframe we wish to clean\n",
    "    \n",
    "    '''\n",
    "    # select necessary columns\n",
    "    data = data[['DATE', 'LATITUDE', 'LONGITUDE', \n",
    "                 'HourlyDryBulbTemperature', 'HourlyPrecipitation', \n",
    "                 'HourlyVisibility', 'HourlyWindSpeed']]\n",
    "    \n",
    "    # drop entries which are all NaNs\n",
    "    # since 'DATE', 'LATITUDE', 'LONGITUDE' has no NaNs, we don't need to consider them when drop NaNs\n",
    "    data = data.set_index(['DATE', 'LATITUDE', 'LONGITUDE']).dropna(how='all').reset_index()\n",
    "    \n",
    "    # convert time to datetime\n",
    "    data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "    \n",
    "    data['HourlyPrecipitation'] = data['HourlyPrecipitation'].apply(lambda x: 0. if x == 'T' else x)\n",
    "    data['HourlyPrecipitation'] = data['HourlyPrecipitation'].apply(normalize_weather_data)\n",
    "    data['HourlyWindSpeed'] = data['HourlyWindSpeed'].apply(float)\n",
    "    data['HourlyDryBulbTemperature'] = data['HourlyDryBulbTemperature'].apply(normalize_weather_data)\n",
    "    data['HourlyVisibility'] = data['HourlyVisibility'].apply(normalize_weather_data)\n",
    "    \n",
    "    data.columns = ['datetime', 'latitude', 'longitude', 'temperature', 'precipitation', 'visibility', 'wind_speed']\n",
    "    \n",
    "    data = data.sort_values('datetime')\n",
    "    data = data.fillna(method='ffill')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bddf7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather_data_daily(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''return the cleaned daily weather data\n",
    "\n",
    "    Keyword argument:\n",
    "    data -- the (hourly) dataframe we wish to group up and clean\n",
    "    \n",
    "    '''\n",
    "    # the method applied while grouping by day for each column\n",
    "    group_method = {\n",
    "        'temperature': 'mean',\n",
    "        'precipitation': 'sum',\n",
    "        'visibility': 'mean',\n",
    "        'wind_speed': 'mean',\n",
    "    }\n",
    "    \n",
    "    daily_data = data.resample('D', on='datetime').agg(group_method)\n",
    "    \n",
    "    daily_data.insert(0, 'latitude', data['latitude'].iloc[0])\n",
    "    daily_data.insert(1, 'longitude', data['longitude'].iloc[0])\n",
    "    \n",
    "    daily_data = daily_data.reset_index()\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''return a tuple cleaned hourly and daily weather data'''\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    weather_csv_files = os.listdir('weather_data')\n",
    "    \n",
    "    for csv_file in tqdm(weather_csv_files, ncols=120, desc='load and clean weather data'):\n",
    "        data = pd.read_csv(Path(WEATHER_DIRECTORY, csv_file), index_col=0)\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(data)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        \n",
    "    # create hourly data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    \n",
    "    daily_data = clean_weather_data_daily(hourly_data)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41778b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load and clean weather data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:05<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "hourly_data, daily_data = load_and_clean_weather_data()\n",
    "\n",
    "hourly_data.to_parquet(Path(CLEAN_DIRECTORY, 'hourly_weather.parquet'))\n",
    "daily_data.to_parquet(Path(CLEAN_DIRECTORY, 'daily_weather.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d51fd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-01 00:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-01-01 01:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-01-01 02:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-01-01 03:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-01 04:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  latitude  longitude  temperature  precipitation  \\\n",
       "0 2008-01-01 00:51:00  40.77898  -73.96925         37.0            NaN   \n",
       "1 2008-01-01 01:51:00  40.77898  -73.96925         37.0            NaN   \n",
       "2 2008-01-01 02:51:00  40.77898  -73.96925         39.0            NaN   \n",
       "3 2008-01-01 03:51:00  40.77898  -73.96925         39.0            NaN   \n",
       "4 2008-01-01 04:51:00  40.77898  -73.96925         39.0            NaN   \n",
       "\n",
       "   visibility  wind_speed  \n",
       "0        10.0         0.0  \n",
       "1         9.0         6.0  \n",
       "2        10.0         6.0  \n",
       "3        10.0         5.0  \n",
       "4         9.0         5.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acd85e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>51.083333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-02</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.250000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-03</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>51.833333</td>\n",
       "      <td>0.43</td>\n",
       "      <td>7.260417</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-04</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>47.636364</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.795455</td>\n",
       "      <td>4.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-05</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>0.44</td>\n",
       "      <td>7.416667</td>\n",
       "      <td>6.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime  latitude  longitude  temperature  precipitation  visibility  \\\n",
       "0 2005-01-01  40.77898  -73.96925    51.083333           0.00    9.875000   \n",
       "1 2005-01-02  40.77898  -73.96925    42.250000           0.00   10.000000   \n",
       "2 2005-01-03  40.77898  -73.96925    51.833333           0.43    7.260417   \n",
       "3 2005-01-04  40.77898  -73.96925    47.636364           0.25    8.795455   \n",
       "4 2005-01-05  40.77898  -73.96925    39.125000           0.44    7.416667   \n",
       "\n",
       "   wind_speed  \n",
       "0    8.666667  \n",
       "1    7.916667  \n",
       "2    5.666667  \n",
       "3    4.363636  \n",
       "4    6.541667  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for hourly weather data\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS HOURLY_WEATHER(\n",
    "    hourly_weather_uid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    datetime DATETIME,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    temperature FLOAT,\n",
    "    precipitation FLOAT,\n",
    "    visibility FLOAT,\n",
    "    wind_speed FLOAT\n",
    ")\"\"\"\n",
    "HOURLY_WEATHER_DTYPE = {\n",
    "    'datetime': db.DateTime(),\n",
    "    'latitude': db.Float(),\n",
    "    'longitude': db.Float(),\n",
    "    'temperature': db.Float(),\n",
    "    'precipitation': db.Float(),\n",
    "    'visibility': db.Float(),\n",
    "    'wind_speed': db.Float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8205cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for daily weather data\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS DAILY_WEATHER(\n",
    "    daily_weather_uid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    datetime DATETIME,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    temperature FLOAT,\n",
    "    precipitation FLOAT,\n",
    "    visibility FLOAT,\n",
    "    wind_speed FLOAT\n",
    ")\"\"\"\n",
    "DAILY_WEATHER_DTYPE = {\n",
    "    'datetime': db.DateTime(),\n",
    "    'latitude': db.Float(),\n",
    "    'longitude': db.Float(),\n",
    "    'temperature': db.Float(),\n",
    "    'precipitation': db.Float(),\n",
    "    'visibility': db.Float(),\n",
    "    'wind_speed': db.Float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70f387c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for taxi data\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS TAXI_TRIPS(\n",
    "    taxi_trips_uid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    dropoff_datetime DATETIME,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    passenger_count INTEGER,\n",
    "    distance FLOAT\n",
    ")\"\"\"\n",
    "TAXI_TRIPS_DTYPE = {\n",
    "    'pickup_datetime': db.DateTime(),\n",
    "    'dropoff_datetime': db.DateTime(),\n",
    "    'pickup_latitude': db.Float(),\n",
    "    'pickup_longitude': db.Float(),\n",
    "    'dropoff_latitude': db.Float(),\n",
    "    'dropoff_longitude': db.Float(),\n",
    "    'fare_amount': db.Float(),\n",
    "    'tip_amount': db.Float(),\n",
    "    'passenger_count': db.Integer(),\n",
    "    'distance': db.Float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31123357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for uber data\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS UBER_TRIPS(\n",
    "    uber_trips_uid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ")\"\"\"\n",
    "UBER_TRIPS_DTYPE = {\n",
    "    'pickup_datetime': db.DateTime(),\n",
    "    'pickup_latitude': db.Float(),\n",
    "    'pickup_longitude': db.Float(),\n",
    "    'dropoff_latitude': db.Float(),\n",
    "    'dropoff_longitude': db.Float(),\n",
    "    'fare_amount': db.Float(),\n",
    "    'tip_amount': db.Float(),\n",
    "    'distance': db.Float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: Dict[str, pd.DataFrame], table_to_dtype_dict: Dict[str, Dict[str, db.types.TypeEngine]]):\n",
    "    '''add dataframes to the database\n",
    "    \n",
    "    Keyword arguments:\n",
    "    table_to_df_dict -- the dictionary that stores the data\n",
    "    table_to_dtype_dict -- the dictionary that stores the corresponding dtypes\n",
    "    '''\n",
    "    with engine.connect() as connection: \n",
    "        for name, data in table_to_df_dict.items():\n",
    "            print(f'Saving data to {name} with entries {data.shape[0]} ...', end=' ')\n",
    "            data.to_sql(name=name, con=connection, if_exists='replace', index=False, dtype=table_to_dtype_dict[name])\n",
    "            print('suceeded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"TAXI_TRIPS\": taxi_data,\n",
    "    \"UBER_TRIPS\": uber_data,\n",
    "    \"HOURLY_WEATHER\": hourly_data,\n",
    "    \"DAILY_WEATHER\": daily_data,\n",
    "}\n",
    "map_table_name_to_dtype = {\n",
    "    \"TAXI_TRIPS\": TAXI_TRIPS_DTYPE,\n",
    "    \"UBER_TRIPS\": UBER_TRIPS_DTYPE,\n",
    "    \"HOURLY_WEATHER\": HOURLY_WEATHER_DTYPE,\n",
    "    \"DAILY_WEATHER\": DAILY_WEATHER_DTYPE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74004f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to TAXI_TRIPS with entries 368766 ... suceeded.\n",
      "Saving data to UBER_TRIPS with entries 194766 ... suceeded.\n",
      "Saving data to HOURLY_WEATHER with entries 195391 ... suceeded.\n",
      "Saving data to DAILY_WEATHER with entries 6550 ... suceeded.\n"
     ]
    }
   ],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe, map_table_name_to_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_N = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_N).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_N, \"some_descriptive_name.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
